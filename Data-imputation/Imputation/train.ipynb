{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as mlt\n",
    "import seaborn as sp\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from datetime import timedelta\n",
    "import torch.autograd.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_label(size, is_real=True, noise_ratio=0.1):\n",
    "    if is_real:\n",
    "        label = torch.ones(size, 1)\n",
    "    else:\n",
    "        label = torch.zeros(size, 1)\n",
    "    return label.to(device)\n",
    "\n",
    "def gen_z_input(batch_size, step, dset, dset_mask):\n",
    "    return [dset[step * batch_size: (step + 1) * batch_size], dset_mask[step * batch_size: (step + 1) * batch_size]]\n",
    "\n",
    "\n",
    "def gen_fake_batch(generator, batch_size, step, dset, dset_mask):\n",
    "    z = gen_z_input(batch_size, step, dset, dset_mask)\n",
    "    fake_dset = generator.predict(z)\n",
    "    fake_label = gen_label(batch_size, is_real=False)\n",
    "    return fake_dset, fake_label\n",
    "\n",
    "\n",
    "def gen_real_batch(batch_size, step, dset):\n",
    "    real_dset = dset[step * batch_size: (step + 1) * batch_size]\n",
    "    real_label = gen_label(batch_size, is_real=True)\n",
    "    return real_dset, real_label\n",
    "\n",
    "def gen_random_batch(batch_size, step, dset):\n",
    "    random_noise = dset[step * batch_size: (step + 1) * batch_size]\n",
    "    return random_noise\n",
    "    \n",
    "def calculate_gradient_penalty(discriminator, real_data, fake_data):\n",
    "    eta = torch.FloatTensor(batch_size, lag_size, input_size).uniform_(0, 1).to(device)\n",
    "    eta = eta.expand(batch_size, lag_size, input_size)\n",
    "\n",
    "    interpolated = eta * real_data + ((1 - eta) * fake_data)\n",
    "\n",
    "    # define it to calculate gradient\n",
    "    interpolated = Variable(interpolated, requires_grad=True)\n",
    "\n",
    "    # calculate probability of interpolated examples\n",
    "    prob_interpolated = discriminator(interpolated)\n",
    "\n",
    "    fake = (torch.ones(prob_interpolated.size()).to(device)).requires_grad_(True)\n",
    "\n",
    "    # calculate gradients of probabilities with respect to examples\n",
    "    gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated, grad_outputs=fake, create_graph=True, retain_graph=True)[0]\n",
    "    gradients = gradients.reshape(batch_size, -1)\n",
    "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "    grad_penalty = ((gradients_norm - 1) ** 2).mean() * lambda_term\n",
    "    return grad_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mmd(real_samples, fake_samples, sigma=1.0):\n",
    "    real_samples_flat = real_samples.view(-1, real_samples.size(-1))\n",
    "    fake_samples_flat = fake_samples.view(-1, fake_samples.size(-1))\n",
    "\n",
    "    kernel_real_real = torch.exp(-torch.cdist(real_samples_flat, real_samples_flat, p=2) / (2 * sigma**2)).mean(dim=0)\n",
    "    kernel_fake_fake = torch.exp(-torch.cdist(fake_samples_flat, fake_samples_flat, p=2) / (2 * sigma**2)).mean(dim=0)\n",
    "    kernel_real_fake = torch.exp(-torch.cdist(real_samples_flat, fake_samples_flat, p=2) / (2 * sigma**2)).mean(dim=0)\n",
    "\n",
    "    mmd_loss = kernel_real_real + kernel_fake_fake - 2 * kernel_real_fake\n",
    "    mmd_loss = torch.mean(mmd_loss)\n",
    "\n",
    "    return mmd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Gan(generator, discriminator, optimizer_discriminator, optimizer_generator, loss_function, loss_function_MSE, real_train, missing_train, mask_train, step_per_epoch):\n",
    "    \n",
    "    errors_discriminator = []\n",
    "    errors_generator = []\n",
    "    real_dataset = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    gen_dataset = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    epoch = 500\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        count = 0\n",
    "        for step in range(step_per_epoch):\n",
    "            # Data for training the discriminator\n",
    "            real_data, real_label = gen_real_batch(batch_size, count, real_train)\n",
    "            z_input, mask_input = gen_z_input(batch_size, count, missing_train, mask_train)\n",
    "\n",
    "            # random_noise = torch.tensor(np.random.randn(batch_size, lag_size, input_size), dtype=torch.float32, requires_grad=True).to(device)\n",
    "            # random_noise = (1 - mask_input)\n",
    "            random_data = np.random.normal(loc=0, scale=1, size=(batch_size, lag_size, input_size))\n",
    "            random_data = np.clip(random_data, 0, 1)\n",
    "            random_data = torch.tensor(random_data,dtype=torch.float32).to(device)\n",
    "            z_input += random_data\n",
    "            fake_data = generator(z_input)\n",
    "            fake_label = gen_label(batch_size, is_real=False)\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            output_discriminator_real = discriminator(real_data)\n",
    "            output_discriminator_fake = discriminator(fake_data)\n",
    "            \n",
    "            #gradient_penalty = calculate_gradient_penalty(discriminator, real_data.detach(), fake_data.detach())\n",
    "            d_loss = loss_function(output_discriminator_real, real_label) + loss_function(output_discriminator_fake, fake_label)\n",
    "            #d_loss = -torch.sum(output_discriminator_real) + torch.sum(output_discriminator_fake)\n",
    "            d_loss.backward()\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            gan_fake_label = gen_label(batch_size, is_real=True)\n",
    "            \n",
    "            #Training the generator\n",
    "            generator.zero_grad()\n",
    "            generated_samples = generator(z_input)\n",
    "            output_discriminator_generated = discriminator(generated_samples)\n",
    "            #L2 = torch.norm((real_data * mask_input) - (generated_samples * mask_input))\n",
    "            #L2 = torch.sum((real_data - generated_samples)**2)\n",
    "            L2 = loss_function_MSE(real_data, generated_samples)\n",
    "            #g_loss = -torch.sum(output_discriminator_generated) + (10 * L2)\n",
    "            g_loss = loss_function(output_discriminator_generated, gan_fake_label) + (L2)\n",
    "            g_loss.backward()\n",
    "            optimizer_generator.step()\n",
    "            errors_discriminator.append(d_loss.item())\n",
    "            errors_generator.append(g_loss.item())\n",
    "\n",
    "            # discriminator.zero_grad()\n",
    "\n",
    "            # output_discriminator_real = discriminator(real_data)\n",
    "            # output_discriminator_fake = discriminator(fake_data)\n",
    "            \n",
    "            # gradient_penalty = calculate_gradient_penalty(discriminator, real_data.detach(), fake_data.detach())\n",
    "\n",
    "            # #d_loss = loss_function(output_discriminator_real, real_label) + loss_function(output_discriminator_fake, fake_label)\n",
    "            # d_loss = -torch.sum(output_discriminator_real) + torch.sum(output_discriminator_fake) + gradient_penalty\n",
    "            # d_loss.backward()\n",
    "            # optimizer_discriminator.step()\n",
    "\n",
    "            # if step % 3 == 0:\n",
    "            #     gan_fake_label = gen_label(batch_size, is_real=True)\n",
    "            \n",
    "            #     # Training the generator\n",
    "            #     generator.zero_grad()\n",
    "            #     generated_samples = generator(z_input)\n",
    "            #     output_discriminator_generated = discriminator(generated_samples)\n",
    "            #     #L2 = torch.norm((real_data * mask_input) - (generated_samples * mask_input))\n",
    "            #     L2 = loss_function(real_data, generated_samples)\n",
    "            #     #mmd_loss = compute_mmd(real_data, generated_samples)\n",
    "            #     g_loss = -torch.sum(output_discriminator_generated) + (1.2 * L2)\n",
    "            #     g_loss.backward()\n",
    "            #     optimizer_generator.step()\n",
    "            #     errors_discriminator.append(d_loss.item())\n",
    "            #     errors_generator.append(g_loss.item())\n",
    "            \n",
    "            count += 1\n",
    "            if i == epoch - 1:\n",
    "                real_dataset = torch.cat([real_dataset, real_data], dim=0)\n",
    "                gen_dataset = torch.cat([gen_dataset, fake_data], dim=0)\n",
    "\n",
    "            # Show loss\n",
    "            if step == step_per_epoch - 1:\n",
    "                print(f\"Epoch: {i} Loss D.: {d_loss.item()} Loss G.: {g_loss.item()} mmd: {L2}\")\n",
    "\n",
    "    return real_dataset, gen_dataset, errors_generator, errors_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Seq2Seq(model, optimizer_model, loss_function, real_train, missing_train, mask_train, step_per_epoch):\n",
    "    \n",
    "    errors_generator = []\n",
    "    real_dataset = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    gen_dataset = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    mask = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    epoch = 500\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        count = 0\n",
    "        for step in range(step_per_epoch):\n",
    "            # Data for training the discriminator\n",
    "            real_data, real_label = gen_real_batch(batch_size, count, real_train)\n",
    "\n",
    "            z_input, mask_input = gen_z_input(batch_size, count, missing_train, mask_train)\n",
    "            #z_input = torch.cat((z_input, (1 - mask_input[:, :, 1]).unsqueeze(2)), dim=2)\n",
    "            model.zero_grad()\n",
    "            generated_samples = model(z_input)\n",
    "\n",
    "            L2 = torch.sum((real_data - generated_samples)**2)\n",
    "            g_loss = loss_function(generated_samples, real_data) + L2\n",
    "            g_loss.backward()\n",
    "            optimizer_model.step()\n",
    "            errors_generator.append(g_loss.item())\n",
    "            \n",
    "            count += 1\n",
    "            if i == epoch - 1:\n",
    "                real_dataset = torch.cat([real_dataset, real_data], dim=0)\n",
    "                gen_dataset = torch.cat([gen_dataset, generated_samples], dim=0)\n",
    "                mask = torch.cat([mask, mask_input], dim=0)\n",
    "\n",
    "            # Show loss\n",
    "            if step == step_per_epoch - 1:\n",
    "                print(f\"Epoch: {i} Loss G.: {g_loss.item()}\")\n",
    "\n",
    "    return real_dataset, gen_dataset, errors_generator, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ConvGan(generator, discriminator, optimizer_discriminator, optimizer_generator, loss_function, real_train, missing_train, mask_train, step_per_epoch, random_data):\n",
    "    \n",
    "    errors_discriminator = []\n",
    "    errors_generator = []\n",
    "    real_dataset = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    gen_dataset = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    mask_dataset = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    epoch = 1000\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        count = 0\n",
    "        for step in range(step_per_epoch):\n",
    "            # Data for training the discriminator\n",
    "            real_data, real_label = gen_real_batch(batch_size, count, real_train)\n",
    "            z_input, mask_input = gen_z_input(batch_size, count, missing_train, mask_train)\n",
    "            random_noise = gen_random_batch(batch_size, count, random_data)\n",
    "            #z_input = z_input + random_noise\n",
    "\n",
    "            #random_noise = torch.tensor(np.random.randn(batch_size, lag_size, input_size), dtype=torch.float32, requires_grad=True).to(device)\n",
    "            # random_noise = (1 - mask_input)\n",
    "\n",
    "            fake_data = generator(real_data, random_noise)\n",
    "            fake_label = gen_label(batch_size, is_real=False)\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # real_data = torch.transpose(real_data, 1, 2)\n",
    "            # fake_data = torch.transpose(fake_data, 1, 2)\n",
    "            output_discriminator_real = discriminator(real_data)\n",
    "            output_discriminator_fake = discriminator(fake_data)\n",
    "            d_loss = loss_function(output_discriminator_real, real_label) + loss_function(output_discriminator_fake, fake_label)\n",
    "            d_loss.backward()\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            # Training the generator\n",
    "            generator.zero_grad()\n",
    "            generated_samples = generator(real_data, random_noise)\n",
    "            output_discriminator_generated = discriminator(generated_samples)\n",
    "            #L2 = torch.norm((real_data * mask_input) - (generated_samples * mask_input))\n",
    "            L2 = torch.sum((real_data - generated_samples)**2)\n",
    "            g_loss = loss_function(output_discriminator_generated, real_label) + L2\n",
    "            g_loss.backward()\n",
    "            optimizer_generator.step()\n",
    "            errors_discriminator.append(d_loss.item())\n",
    "            errors_generator.append(g_loss.item())\n",
    "            \n",
    "            # with torch.backends.cudnn.flags(enabled=False):\n",
    "            #     discriminator.zero_grad()\n",
    "\n",
    "            #     output_discriminator_real = discriminator(real_data)\n",
    "            #     output_discriminator_fake = discriminator(fake_data)\n",
    "            \n",
    "            #     gradient_penalty = calculate_gradient_penalty(discriminator, real_data.detach(), fake_data.detach())\n",
    "\n",
    "            #     #d_loss = loss_function(output_discriminator_real, real_label) + loss_function(output_discriminator_fake, fake_label)\n",
    "            #     d_loss = -torch.sum(output_discriminator_real) + torch.sum(output_discriminator_fake) + gradient_penalty\n",
    "            #     d_loss.backward()\n",
    "            #     optimizer_discriminator.step()\n",
    "\n",
    "            # if step % 7 == 0:\n",
    "            #     gan_fake_label = gen_label(batch_size, is_real=True)\n",
    "            \n",
    "            #     # Training the generator\n",
    "            #     generator.zero_grad()\n",
    "            #     generated_samples = generator(random_noise)\n",
    "            #     output_discriminator_generated = discriminator(generated_samples)\n",
    "            #     #L2 = torch.norm((real_data * mask_input) - (generated_samples * mask_input))\n",
    "            #     L2 = torch.sum((real_data[:, -1, 0].view(-1) - generated_samples[:, -1, 0].view(-1))**2)\n",
    "            #     g_loss = -torch.sum(output_discriminator_generated) + L2\n",
    "            #     g_loss.backward()\n",
    "            #     optimizer_generator.step()\n",
    "            #     errors_discriminator.append(d_loss.item())\n",
    "            #     errors_generator.append(g_loss.item())\n",
    "\n",
    "            count += 1\n",
    "            if i == epoch - 1:\n",
    "                real_dataset = torch.cat([real_dataset, real_data], dim=0)\n",
    "                gen_dataset = torch.cat([gen_dataset, fake_data], dim=0)\n",
    "                mask_dataset = torch.cat([mask_dataset, mask_input], dim=0)\n",
    "\n",
    "            # Show loss\n",
    "            if step == step_per_epoch - 1:\n",
    "                print(f\"Epoch: {i} Loss D.: {d_loss.item()} Loss G.: {g_loss.item()}\")\n",
    "\n",
    "    return real_dataset, gen_dataset, errors_generator, errors_discriminator, mask_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoEncoder(model, optimizer_model, loss_function, real_train, missing_train, mask_train, step_per_epoch):\n",
    "    \n",
    "    errors_generator = []\n",
    "    real_dataset = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    gen_dataset = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    mask = torch.empty((0, lag_size, input_size), dtype=torch.float32).to(device)\n",
    "    epoch = 500\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        count = 0\n",
    "        for step in range(step_per_epoch):\n",
    "            # Data for training the discriminator\n",
    "            real_data, real_label = gen_real_batch(batch_size, count, real_train)\n",
    "\n",
    "            z_input, mask_input = gen_z_input(batch_size, count, missing_train, mask_train)\n",
    "            #z_input = torch.cat((z_input, (1 - mask_input[:, :, 1]).unsqueeze(2)), dim=2)\n",
    "            model.zero_grad()\n",
    "            recon_x, mean, logvar = model(z_input)\n",
    "\n",
    "            L2 = torch.sum((real_data - recon_x)**2)\n",
    "            # g_loss = loss_function(generated_samples, real_data) + L2\n",
    "            BCE_loss = loss_function(recon_x, real_data)\n",
    "            KLD_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())) * 0.00025\n",
    "            g_loss = BCE_loss + (KLD_loss)\n",
    "            g_loss.backward()\n",
    "            optimizer_model.step()\n",
    "            errors_generator.append(g_loss.item())\n",
    "            \n",
    "            count += 1\n",
    "            if i == epoch - 1:\n",
    "                real_dataset = torch.cat([real_dataset, real_data], dim=0)\n",
    "                gen_dataset = torch.cat([gen_dataset, recon_x], dim=0)\n",
    "                mask = torch.cat([mask, mask_input], dim=0)\n",
    "\n",
    "            # Show loss\n",
    "            if step == step_per_epoch - 1:\n",
    "                print(f\"Epoch: {i} Loss G.: {g_loss.item()}\")\n",
    "\n",
    "    return real_dataset, gen_dataset, errors_generator, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
