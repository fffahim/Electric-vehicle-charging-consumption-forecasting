{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mogrifier_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, future_step, batch_size, layer_size = 2):\n",
    "        super(Mogrifier_LSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.future_step = future_step\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.mogrifier_lstm_layer1 = MogrifierLSTMCell(input_size, hidden_size, layer_size)\n",
    "        self.mogrifier_lstm_layer2 = MogrifierLSTMCell(hidden_size, hidden_size, layer_size)\n",
    "\n",
    "        # Backward LSTM layer\n",
    "        self.mogrifier_lstm_layer1_backward = MogrifierLSTMCell(input_size, hidden_size, layer_size)\n",
    "        self.mogrifier_lstm_layer2_backward = MogrifierLSTMCell(hidden_size, hidden_size, layer_size)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size * lag_size, future_step * input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h1, c1 = [torch.zeros(self.batch_size, self.hidden_size).to(device), torch.zeros(self.batch_size, self.hidden_size).to(device)]\n",
    "        h2, c2 = [torch.zeros(self.batch_size, self.hidden_size).to(device), torch.zeros(self.batch_size, self.hidden_size).to(device)]\n",
    "        \n",
    "        h1_backward, c1_backward = torch.zeros(self.batch_size, self.hidden_size).to(device), torch.zeros(self.batch_size, self.hidden_size).to(device)\n",
    "        h2_backward, c2_backward = torch.zeros(self.batch_size, self.hidden_size).to(device), torch.zeros(self.batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        hidden_states_forward = []\n",
    "        hidden_states_backward = []\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(lag_size):\n",
    "            tempx = x[:, i]\n",
    "            h1,c1 = self.mogrifier_lstm_layer1(tempx, (h1, c1))     \n",
    "            #h2,c2 = self.mogrifier_lstm_layer2(h1, (h2, c2))\n",
    "            #h1 = self.dropout(h1)\n",
    "            #out = self.linear1(h1)\n",
    "            hidden_states_forward.append(h1.unsqueeze(1))\n",
    "            #outputs.append(out.unsqueeze(1)) \n",
    "\n",
    "        # for i in reversed(range(x.size(1))):\n",
    "        #     tempx = x[:, i]\n",
    "        #     h1_backward, c1_backward = self.mogrifier_lstm_layer1_backward(tempx, (h1_backward, c1_backward))     \n",
    "        #     #h2_backward, c2_backward = self.mogrifier_lstm_layer2_backward(h1_backward, (h2_backward, c2_backward))\n",
    "        #     #out = self.linear1(h1_backward)\n",
    "        #     hidden_states_backward.append(h2.unsqueeze(1))\n",
    "        hidden_states_forward = torch.cat(hidden_states_forward, dim=1)\n",
    "        # hidden_states_backward = torch.cat(hidden_states_backward, dim=1)\n",
    "\n",
    "        # Concatenate forward and backward hidden states\n",
    "        # hidden_states = torch.cat((hidden_states_forward, hidden_states_backward), dim=-1)\n",
    "        #hidden_states = torch.flatten(hidden_states[:, -1, :])\n",
    "        outputs = self.linear(hidden_states_forward.view(self.batch_size, -1))  # Only take the last time step output\n",
    "        #outputs = self.linear(hidden_states_forward[:, -1, :])  # Only take the last time step output\n",
    "        outputs = outputs.view(self.batch_size, self.future_step, -1)\n",
    "        # hidden_states = torch.cat(hidden_states, dim = 1)  \n",
    "        # outputs = torch.cat(outputs, dim = 1)  \n",
    "        # outputs = outputs[:, -1, :]     \n",
    "        \n",
    "        return self.relu(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MogrifierLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, mogrify_steps):\n",
    "        super(MogrifierLSTMCell, self).__init__()\n",
    "        self.mogrify_steps = mogrify_steps\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.mogrifier_list = nn.ModuleList([nn.Linear(hidden_size, input_size)])\n",
    "        for i in range(1, mogrify_steps):\n",
    "            if i % 2 == 0:\n",
    "                self.mogrifier_list.extend([nn.Linear(hidden_size, input_size)])\n",
    "            else:\n",
    "                self.mogrifier_list.extend([nn.Linear(input_size, hidden_size)])\n",
    "   \n",
    "    def mogrify(self, x, h):\n",
    "        for i in range(self.mogrify_steps):\n",
    "            if (i+1) % 2 == 0: \n",
    "                h = (2*torch.sigmoid(self.mogrifier_list[i](x))) * h\n",
    "            else:\n",
    "                x = (2*torch.sigmoid(self.mogrifier_list[i](h))) * x\n",
    "        return x, h\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        ht, ct = states\n",
    "        x, ht = self.mogrify(x, ht)\n",
    "        ht, ct = self.lstm(x, (ht, ct))\n",
    "        return ht, ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
